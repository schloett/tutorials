{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SkipGram with Negative Sampling (SGNS)\n",
    "\n",
    "\n",
    "## History\n",
    "The SkipGram Model with negative sampling (also know as Word2Vec) was introduced by Mikolov et al. [1,2] in 2013.\n",
    "SkipGramModel implements the skipgram with negative sampling architecture from word2vec (an input word should predict an output word in the context). However, the core component dates back to the 50ies. It is based on the \"distributional hyposthesis\"[3], which states that words in a similar context tend to have a similar meaning.\n",
    "Also the idea of learning word embeddings had been around before [4], but high computational costs hindered their widespread. Mikolov presented a shallow model, which could still learn good representations, but at low computational costs. This and the open release of the implementation can be seen as the success factors of Word2Vec. The widespread most likely got an additional boost by the Python implementation of Radim Řehůřek [gensim](https://radimrehurek.com/gensim/).\n",
    "\n",
    "[1] Mikolov, Tomas, et al. \"Efficient estimation of word representations in vector space.\" ICLR. 2013.  \n",
    "[2] Mikolov, Tomas, et al. \"Distributed representations of words and phrases and their compositionality.\" Advances in neural information processing systems. 2013.  \n",
    "[3] Harris, Zellig S. \"Distributional structure.\" Word 10.2-3 (1954): 146-162.  \n",
    "[4] Bengio, Yoshua, et al. \"A neural probabilistic language model.\" Journal of machine learning research 3.Feb (2003): 1137-1155."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The goal is to find an embedding for the words $\\phi : w \\in W \\rightarrow \\mathbb{R}^{|W| \\times d}$, \n",
    "where $d << |W|$.\n",
    "We want to embed the words into a lower-dimensional, real-valued space, while still retaining information from original space.\n",
    "Given a word $u$, we aim to estimate the likelihood to see another word $v$ co-occur:\n",
    "$p(w_v|w_u)$\n",
    "\n",
    "We can move towards this goal, by maximizing the likelihood of words in the context:  \n",
    "$\\sum_{u \\in W} log P(v \\in C^+|\\phi(u))$  \n",
    "That is, we aim to maximize the log-probability of observing a word $v$ that resides in the context (denoted as $C^+$) of $u$, conditioned on its feature representation $\\phi(u)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Context - Sliding Window\n",
    "We move a sliding window with pre-determined size across a sentence. The middle word is our word of interest and the words to the left and right, which are still within the window are considered as context. We generate context pairs from this sliding window approach. To put more weight on the immediate neighbors, Mikolov proposed to randomly reduce the window size to sample far distant words less often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sliding_window(sentence,window_size=2,downsample=True):\n",
    "    pairs = []\n",
    "    words = [w for w in sentence]\n",
    "    for pos, word in enumerate(words):\n",
    "        reduction = 0\n",
    "        if downsample:\n",
    "            reduction = np.random.randint(window_size)\n",
    "        start = max(0, pos - window_size + reduction)\n",
    "        for pos2, word2 in enumerate(words[start:(pos + window_size + 1 - reduction)], start):\n",
    "            if pos2 != pos:\n",
    "                pairs.append((word, word2))        \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'cat'),\n",
       " ('the', 'sat'),\n",
       " ('cat', 'the'),\n",
       " ('cat', 'sat'),\n",
       " ('cat', 'on'),\n",
       " ('sat', 'the'),\n",
       " ('sat', 'cat'),\n",
       " ('sat', 'on'),\n",
       " ('sat', 'the'),\n",
       " ('on', 'cat'),\n",
       " ('on', 'sat'),\n",
       " ('on', 'the'),\n",
       " ('on', 'mat'),\n",
       " ('the', 'sat'),\n",
       " ('the', 'on'),\n",
       " ('the', 'mat'),\n",
       " ('mat', 'on'),\n",
       " ('mat', 'the')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliding_window(\"the cat sat on the mat\".split(),downsample=False,window_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Shallow Neural Network\n",
    "We can now use the source-context word pairs to train a neural network with our previously defined optimization objective. We model the conditional log-likelihood of every source-context pair as a softmax unit, parametrized by a dot product of the words' feature representations:\n",
    "\n",
    "$P(v^+|\\phi(u)) = \\frac{exp(\\langle \\phi^\\prime(v^+), \\phi(u) \\rangle)}{\\sum_{v \\in W} exp(\\langle \\phi^\\prime(v), \\phi(u) \\rangle)}$\n",
    "\n",
    "where $\\langle \\cdot \\rangle$ is the dot product and $\\phi^\\prime$ is a similar mapping as $\\phi$, often referred to as projection (opposed to embedding). Technically, this is implemented by a shallow neural network, with a linear hidden layer, where the embeddings are the weights between the input layer and the hidden layer and the projections are the weights between the hidden layer and the output layer.\n",
    "\n",
    "However, the normalization in the denominator is costly to compute and therefore, we replace $P(v^+|\\phi(u))$ by negative sampling, as proposed by Mikolov et al. \n",
    "\n",
    "$ log \\ p(w_v|w_u) \\approx log \\ \\sigma(\\langle \\phi^\\prime(v),\\phi(u)\\rangle) + \\sum_{k=1}^{K} \\mathbb{E}_{w_k \\sim P_n(w)}[log \\ \\sigma (-\\langle \\phi^\\prime(w_k),\\phi(u)\\rangle]$  \n",
    "with  \n",
    "$\\langle \\cdotp \\rangle$ the dot product  \n",
    "$\\sigma(x) = \\frac{1}{1+exp(-x)}$ the logistic sigmoid function  \n",
    "$K$ the number of negative samples, drawn from the sampling distribution  \n",
    "$P_n(w), \\forall w \\in W$ where  \n",
    "$W$ is the vocabularity and the probability of drawing a word should correspond to it's frequency. \n",
    "\n",
    "This substitution of the softmax by negative sampling is possible, as we are not interested in actual probabilities, but only aim to find a good embedding. In fact, Mikolov et al. showed empirically that negative sampling yields even better embeddings than the softmax.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SkipGram Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SkipGramNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size_u, emb_size_v, emb_dimension):\n",
    "        super(SkipGramNetwork, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(emb_size_u, emb_dimension,sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(emb_size_v, emb_dimension,sparse=True)\n",
    "        self.init_emb()\n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "        \n",
    "    def forward(self, pos_u, pos_v, neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        emb_v = self.v_embeddings(pos_v)\n",
    "        emb_n = self.v_embeddings(neg_v)\n",
    "        pos_score = F.logsigmoid(torch.mm(emb_v,torch.t(emb_u)))\n",
    "        neg_score = F.logsigmoid(torch.mm(emb_n,torch.t(emb_u)).neg())\n",
    "        score = torch.sum(pos_score) + torch.sum(neg_score)\n",
    "        return -1 * score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Utility\n",
    "In order to prepare our data to be fed into the neural network, we need to:\n",
    "- transform words to integer ids\n",
    "- create context pairs by sliding window\n",
    "- downsample high frequent words (see below)\n",
    "- setup the noise distribution for sampling the negative words (Mikolov empirically showed, that the unigram distribution raised to the power of 0.75 is a good choice)\n",
    "\n",
    "**Downsampling high frequent words**\n",
    "High frequent words are words like \"a\", \"the\", ... (stopwords). They usually provide less information than rare words, but due to their frequency see way more updates. To counter this imbalance, each word in the training set is discarded with probability\n",
    "$P(w_i)= 1-\\sqrt{\\frac{t}{f(w_i)}}$  \n",
    "where $f(w_i)$ is the frequency of the word and $t$ a threshold, typically around $10^{-5}$. This formula aggressively downsamples high frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import random\n",
    "\n",
    "class WVDataset():\n",
    "    def __init__(self, sentences, power=0.75,window_size=5,neg_samples=5,sample=1e-3,min_count=5):\n",
    "        self.sentences = sentences\n",
    "        self.window_size = window_size\n",
    "        self.neg_samples = neg_samples\n",
    "        self.vocab = None\n",
    "        self.index2source = dict()\n",
    "        self.source2index = dict()\n",
    "        self.index2context = dict()\n",
    "        self.context2index = dict()\n",
    "        self.build_vocab(sentences,min_count)\n",
    "        self.ctx_weights = self.make_freq_table(power)\n",
    "        self.neg_sample_buffer = []\n",
    "        # downsample frequent words\n",
    "        self.downsample_probs = np.zeros_like(self.ctx_weights)\n",
    "        if sample > 0:\n",
    "            self.downsample_probs = 1 - np.sqrt(sample/self.ctx_weights).clip(0,1)\n",
    "       \n",
    "        \n",
    "    def build_vocab(self,sentences,min_count):\n",
    "        # word freqency\n",
    "        raw_vocab = defaultdict(int)\n",
    "        for sent in sentences:\n",
    "            for word in sent:\n",
    "                raw_vocab[word] += 1 \n",
    "        # only keep words that occur at least min_count times\n",
    "        self.vocab = {k:v for k,v in raw_vocab.items() if v >= min_count}\n",
    "        del raw_vocab\n",
    "        # map each word to an integer\n",
    "        for word in self.vocab:\n",
    "            self.source2index[word] = len(self.source2index)\n",
    "        # reverse mapping (integer to word)\n",
    "        self.index2source = dict(zip(self.source2index.values(), self.source2index.keys()))\n",
    "        # in- and output layer are the same in word2vec\n",
    "        self.context2index = self.source2index\n",
    "        self.index2context = self.source2index\n",
    "            \n",
    "        \n",
    "    def make_freq_table(self, power):\n",
    "        # the unigram distribution raised to 0.75 empirically performed best as negative sampling distribution\n",
    "        pow_frequency = np.array([self.vocab[self.index2source[i]] for i in range(len(self.vocab))])**power\n",
    "        return pow_frequency / pow_frequency.sum()\n",
    "        \n",
    "\n",
    "    def sliding_window(self, words):\n",
    "        for pos, word in enumerate(words):\n",
    "            # sliding window (randomly reduced to give more weight to closeby words)\n",
    "            reduction = np.random.randint(self.window_size)\n",
    "            start = max(0, pos - self.window_size + reduction)\n",
    "            for pos2, word2 in enumerate(words[start:(pos + self.window_size + 1 - reduction)], start):\n",
    "                if pos2 != pos:\n",
    "                    yield self.source2index[word],self.source2index[word2]\n",
    "    \n",
    "    def negative_sampling(self,context):\n",
    "        neg = []\n",
    "        while len(neg) < self.neg_samples:\n",
    "            try:\n",
    "                negative = self.neg_sample_buffer.pop()\n",
    "                # avoid that context word occurs in negative samples\n",
    "                if negative != context:\n",
    "                    neg.append(negative)\n",
    "            except IndexError:\n",
    "                # instead of sampling negative words for each context pair individually, we sample a large set of negative samples at once (faster)\n",
    "                self.neg_sample_buffer = list(np.random.choice(list(self.index2source.keys()),size=1000,replace=True,p=self.ctx_weights))\n",
    "        return neg\n",
    "        \n",
    "    \n",
    "    def get_pairs(self):\n",
    "        for sent in self.sentences:\n",
    "            # randomly reject frequent words according to downsampling probability\n",
    "            words = [w for w in sent if w in self.vocab and self.downsample_probs[self.source2index[w]] < random.random()]\n",
    "\n",
    "            for source,context in self.sliding_window(words):\n",
    "                yield source,context,self.negative_sampling(context)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "sents = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the cat sat on the table\",\n",
    "    \"the fox sat on the mat\",\n",
    "    \"a car was broken\",\n",
    "    \"the fox sat on the table\"\n",
    "]\n",
    "\n",
    "\n",
    "test_sents = [list(tokenize(s)) for i in range(10) for s in sents] # tokenize and repeat to increase corpus size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'mat'],\n",
       " ['a', 'car', 'was', 'broken'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'mat'],\n",
       " ['a', 'car', 'was', 'broken'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'mat'],\n",
       " ['a', 'car', 'was', 'broken'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'mat'],\n",
       " ['a', 'car', 'was', 'broken'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'mat'],\n",
       " ['a', 'car', 'was', 'broken'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'mat'],\n",
       " ['a', 'car', 'was', 'broken'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'mat'],\n",
       " ['a', 'car', 'was', 'broken'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'mat'],\n",
       " ['a', 'car', 'was', 'broken'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'mat'],\n",
       " ['a', 'car', 'was', 'broken'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'cat', 'sat', 'on', 'the', 'table'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'mat'],\n",
       " ['a', 'car', 'was', 'broken'],\n",
       " ['the', 'fox', 'sat', 'on', 'the', 'table']]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 80,\n",
       " 'cat': 20,\n",
       " 'sat': 40,\n",
       " 'on': 40,\n",
       " 'mat': 20,\n",
       " 'table': 20,\n",
       " 'fox': 20,\n",
       " 'a': 10,\n",
       " 'car': 10,\n",
       " 'was': 10,\n",
       " 'broken': 10}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = WVDataset(test_sents)\n",
    "test_data.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22500645, 0.07955179, 0.13378963, 0.13378963, 0.07955179,\n",
       "       0.07955179, 0.07955179, 0.04730178, 0.04730178, 0.04730178,\n",
       "       0.04730178])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.ctx_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'cat': 1,\n",
       " 'sat': 2,\n",
       " 'on': 3,\n",
       " 'mat': 4,\n",
       " 'table': 5,\n",
       " 'fox': 6,\n",
       " 'a': 7,\n",
       " 'car': 8,\n",
       " 'was': 9,\n",
       " 'broken': 10}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.source2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNS():\n",
    "    def __init__(self, data,dim=100,alpha=0.025, iterations=5):\n",
    "        self.alpha = alpha\n",
    "        self.dim = dim\n",
    "        self.data = data\n",
    "        self.model = SkipGramNetwork(len(self.data.index2source), len(self.data.index2context), self.dim)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=alpha)\n",
    "        self.iterations = iterations\n",
    "        self.train() \n",
    "        self.embs = self.get_embedding()\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.iterations):    \n",
    "            epoch_loss = 0\n",
    "            epoch_pairs = 0\n",
    "            for (pos_u,pos_v,neg_v) in list(self.data.get_pairs()):\n",
    "                print(epoch_pairs,end='\\r')\n",
    "                epoch_pairs += 1\n",
    "                pos_u = torch.LongTensor([pos_u])\n",
    "                pos_v = torch.LongTensor([pos_v])\n",
    "                neg_v = torch.LongTensor(neg_v)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_u, pos_v, neg_v)\n",
    "                epoch_loss += loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "            print(\"{0:d} epoch of {1:d}, {2:d} pairs, loss: {3:f}\".format(epoch+1,self.iterations,epoch_pairs, epoch_loss))\n",
    "      \n",
    "    def get_embedding(self):\n",
    "        e = dict()\n",
    "        embedding = self.model.u_embeddings.weight.data.numpy()\n",
    "        for i in range(len(self.data.index2source)):\n",
    "            e[self.data.index2source[i]] = embedding[i]\n",
    "        return e\n",
    "    \n",
    "    def most_similar(self,to_test,top_n=10):\n",
    "        emb_test = self.model.u_embeddings(torch.LongTensor([self.data.source2index[to_test]]))\n",
    "        score = torch.mm(self.model.u_embeddings.weight.data, torch.t(emb_test))\n",
    "        norms_emb = torch.norm(self.model.u_embeddings.weight, dim=1)\n",
    "        normalization_factors =  norms_emb * torch.norm(emb_test)\n",
    "        scores = score.squeeze()/normalization_factors\n",
    "        values, indices = scores.sort(descending=True)\n",
    "        values = values.detach().numpy()\n",
    "        indices = indices.detach().numpy()\n",
    "        if top_n < 0 or top_n > len(self.data.index2source):\n",
    "            top_n = len(self.data.index2source)\n",
    "        for i in range(top_n):\n",
    "            print(self.data.index2source[indices[i]],':',values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch of 5, 654 pairs, loss: 2667.565918\n",
      "2 epoch of 5, 640 pairs, loss: 1913.859619\n",
      "3 epoch of 5, 624 pairs, loss: 1589.992676\n",
      "4 epoch of 5, 634 pairs, loss: 1508.803833\n",
      "5 epoch of 5, 637 pairs, loss: 1481.087280\n",
      "cat : 1.0000001\n",
      "fox : 0.9931775\n",
      "on : 0.9506307\n",
      "mat : 0.861469\n",
      "table : 0.8440743\n",
      "a : 0.6433477\n",
      "sat : 0.63946944\n",
      "broken : 0.63221645\n",
      "car : 0.473864\n",
      "was : 0.44934615\n"
     ]
    }
   ],
   "source": [
    "test_data = WVDataset(test_sents,window_size=2,neg_samples=5,sample=0)\n",
    "model = SGNS(test_data,alpha=0.025,iterations=5,dim=5)\n",
    "model.most_similar('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "m2 = Word2Vec(test_sents, sample=0,compute_loss=True ,min_count=0,vector_size=100,alpha=0.025,min_alpha=0.025,workers=1,sg=1,hs=0,epochs=5,window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fox', 0.9956952333450317),\n",
       " ('on', 0.9640274047851562),\n",
       " ('mat', 0.8453554511070251),\n",
       " ('table', 0.8081494569778442),\n",
       " ('broken', 0.8074371814727783),\n",
       " ('a', 0.7790175676345825),\n",
       " ('was', 0.693585991859436),\n",
       " ('car', 0.6879597306251526),\n",
       " ('sat', 0.5816310048103333),\n",
       " ('the', 0.43878644704818726)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.wv.most_similar(positive='cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to 20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 13.7M  100 13.7M    0     0  1908k      0  0:00:07  0:00:07 --:--:-- 2209k00:40 --:--:--  0:00:40  344k\n"
     ]
    }
   ],
   "source": [
    "# shell scripts for downloading the data and placing it in a corresponding directory\n",
    "!mkdir newsgroups\n",
    "!curl -o newsgroups/news.tar.gz \"http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\"\n",
    "# extract the files\n",
    "!gzip -d < newsgroups/news.tar.gz | tar xf - --directory newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "\n",
    "twenty_train = load_files('./newsgroups/20news-bydate-train/', encoding='latin1')\n",
    "twenty_test = load_files('./newsgroups/20news-bydate-test/', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: cubbie@garnet.berkeley.edu (                               )\\nSubject: Re: Cubs behind Marlins? How?\\nArticle-I.D.: agate.1pt592$f9a\\nOrganization: University of California, Berkeley\\nLines: 12\\nNNTP-Posting-Host: garnet.berkeley.edu\\n\\n\\ngajarsky@pilot.njin.net writes:\\n\\nmorgan and guzman will have era's 1 run higher than last year, and\\n the cubs will be idiots and not pitch harkey as much as hibbard.\\n castillo won't be good (i think he's a stud pitcher)\\n\\n       This season so far, Morgan and Guzman helped to lead the Cubs\\n       at top in ERA, even better than THE rotation at Atlanta.\\n       Cubs ERA at 0.056 while Braves at 0.059. We know it is early\\n       in the season, we Cubs fans have learned how to enjoy the\\n       short triumph while it is still there.\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1886"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 10 # use only every step-th document\n",
    "train_reduced = dict()\n",
    "test_reduced = dict()\n",
    "train_reduced['data'] = [twenty_train.data[i] for i in range(0,len(twenty_train.data),step)]\n",
    "train_reduced['target'] = [twenty_train.target[i] for i in range(0,len(twenty_train.data),step)]\n",
    "test_reduced['data'] = [twenty_test.data[i] for i in range(0,len(twenty_test.data),step)]\n",
    "test_reduced['target'] = [twenty_test.target[i] for i in range(0,len(twenty_test.data),step)]\n",
    "\n",
    "raw_docs = train_reduced['data'] + test_reduced['data']\n",
    "docs = [list(tokenize(d,lowercase=True)) for d in raw_docs]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 37.1 ms, total: 10.7 s\n",
      "Wall time: 3.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gensim_m = Word2Vec(docs,alpha=0.025, min_alpha=0.025,sg=1,hs=0,epochs=5,vector_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('motherboard', 0.8644053339958191),\n",
       " ('mhz', 0.8494293093681335),\n",
       " ('isa', 0.8344085216522217),\n",
       " ('mounting', 0.8319534659385681),\n",
       " ('cache', 0.8281416893005371),\n",
       " ('viper', 0.8237494230270386),\n",
       " ('maxtor', 0.8235769271850586),\n",
       " ('jumper', 0.8224573135375977),\n",
       " ('seagate', 0.8213753700256348),\n",
       " ('socket', 0.8191305994987488)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_m.wv.most_similar(positive='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch of 5, 2509273 pairs, loss: 6315909.500000\n",
      "2 epoch of 5, 2506032 pairs, loss: 5676597.000000\n",
      "3 epoch of 5, 2507857 pairs, loss: 5518800.500000\n",
      "4 epoch of 5, 2506791 pairs, loss: 5407366.000000\n",
      "5 epoch of 5, 2506609 pairs, loss: 5323939.500000\n",
      "CPU times: user 2h 6min 40s, sys: 26min 2s, total: 2h 32min 43s\n",
      "Wall time: 2h 28min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "m = SGNS(WVDataset(docs),dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu : 1.0000001\n",
      "motherboard : 0.89053804\n",
      "socket : 0.87638056\n",
      "seagate : 0.87345856\n",
      "isa : 0.8704405\n",
      "ide : 0.8703649\n",
      "meg : 0.8691706\n",
      "megs : 0.8687872\n",
      "floppies : 0.866265\n",
      "maxtor : 0.8640162\n"
     ]
    }
   ],
   "source": [
    "m.most_similar('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "<div class=\"alert alert-info\">\n",
    "    <ul>\n",
    "<li>Besides the SkipGram Model, Mikolov introduced a second model, called \"Continuous Bag of Words\" (CBOW). There, the goal is to predict the current word from the context. The CBOW model is basically the opposite to SkipGram. In SkipGram, we predict the context words (the words in the window), in CBOW, we predict the word in the middle, given the context words (words in the window).</li>\n",
    "<li>The implementation here is only used for demonstration purposes and not optimized for performance. Hence, it is horribly slow. Also it does not care about memory consumption.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec\n",
    "The SkipGram Model can not only be used to embed words, but also complete sentences, paragraph or documents [1]. The model, corresponding to Skipgram is called PV-DBOW (Paragraph Vector Distributed Bag of Words) in [1]. Given a document, we aim to predict its context, i.e. the words in this document. Documents with similar context (i.e. documents that share many words) should be close in embedding space.  \n",
    "The second model introduced in [1] is called distributed memory (DM). It works similar to CBOW mentioned above, but we add the document to the context. That means we slide a window over the sentences in the document and try to predict the word in the middle of this window, given the remaining words + the document. As the document information is the same for all word windows, this can be seen as some kind of memory and provided the name distributed memory.\n",
    "\n",
    "[1] Le, Quoc, and Tomas Mikolov. \"Distributed representations of sentences and documents.\" International conference on machine learning. 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network architecture stays the same as before, we only change the input layer from words to documents. That is, we do not need to change the computation within our model, but only adapt the data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import random\n",
    "\n",
    "class DVDataset():\n",
    "    def __init__(self, documents, power=0.75,neg_samples=5,sample=1e-3,min_count=5):\n",
    "        self.documents = documents\n",
    "        self.neg_samples = neg_samples\n",
    "        self.vocab = None\n",
    "        self.word_vocab = None\n",
    "        self.index2source = dict() #doc2index\n",
    "        self.source2index = dict() #index2doc\n",
    "        self.index2context = dict() #index2word\n",
    "        self.context2index = dict() #word2index\n",
    "        self.build_vocab(documents,min_count)\n",
    "        self.ctx_weights = self.make_freq_table(power)\n",
    "        self.neg_sample_buffer = []\n",
    "       \n",
    "            \n",
    "    def build_vocab(self,documents,min_count):\n",
    "        self.vocab = defaultdict(int)\n",
    "        # word freqency\n",
    "        raw_vocab = defaultdict(int)\n",
    "        for d in documents:\n",
    "            for tag in d.tags:\n",
    "                self.vocab[tag] += 1\n",
    "            for word in d.words:\n",
    "                raw_vocab[word] += 1 \n",
    "        # only keep words that occur at least min_count times\n",
    "        self.word_vocab = {k:v for k,v in raw_vocab.items() if v >= min_count}\n",
    "        del raw_vocab\n",
    "        \n",
    "        # map each word to an integer\n",
    "        for word in self.word_vocab:\n",
    "            self.context2index[word] = len(self.context2index)\n",
    "        # reverse mapping (integer to word)\n",
    "        self.index2context = dict(zip(self.context2index.values(), self.context2index.keys()))\n",
    "        \n",
    "        # map each document to an integer\n",
    "        for doc in self.vocab:\n",
    "            self.source2index[doc] = len(self.source2index)\n",
    "        # reverse mapping (integer to document)\n",
    "        self.index2source = dict(zip(self.source2index.values(), self.source2index.keys()))\n",
    "            \n",
    "        \n",
    "    def make_freq_table(self, power):\n",
    "        # the unigram distribution raised to 0.75 empirically performed best as negative sampling distribution\n",
    "        pow_frequency = np.array([self.word_vocab[self.index2context[i]] for i in range(len(self.word_vocab))])**power\n",
    "        return pow_frequency / pow_frequency.sum()\n",
    "    \n",
    "    def negative_sampling(self,context):\n",
    "        neg = []\n",
    "        while len(neg) < self.neg_samples:\n",
    "            try:\n",
    "                negative = self.neg_sample_buffer.pop()\n",
    "                # avoid that context word occurs in negative samples\n",
    "                if negative != context:\n",
    "                    neg.append(negative)\n",
    "            except IndexError:\n",
    "                self.neg_sample_buffer = list(np.random.choice(list(self.index2context.keys()),size=1000,replace=True,p=self.ctx_weights))\n",
    "        return neg\n",
    "        \n",
    "    \n",
    "    def get_pairs(self):\n",
    "        for d in self.documents:\n",
    "            # randomly reject frequent words according to downsampling probability\n",
    "            words = [self.context2index[w] for w in d.words if w in self.word_vocab]\n",
    "\n",
    "            for tag in d.tags:\n",
    "                for w in words:\n",
    "                    yield self.source2index[tag],w,self.negative_sampling(w)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data\n",
    "Create a `TaggedDocument` Object for each documents. This object has two attributes:\n",
    "- words: a list of the words in the document\n",
    "- tags: a list of document identifiers (usually a single element list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "doc_id = 0\n",
    "tagged_docs = []\n",
    "for d in train_reduced['data'] + test_reduced['data']:\n",
    "    tagged_docs.append(TaggedDocument(list(tokenize(d,lowercase=True)),[str(doc_id)]))\n",
    "    doc_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch of 10, 580512 pairs, loss: 1752520.375000\n",
      "2 epoch of 10, 580512 pairs, loss: 1428844.125000\n",
      "3 epoch of 10, 580512 pairs, loss: 1348997.250000\n",
      "4 epoch of 10, 580512 pairs, loss: 1306878.000000\n",
      "5 epoch of 10, 580512 pairs, loss: 1269509.125000\n",
      "6 epoch of 10, 580512 pairs, loss: 1231818.750000\n",
      "7 epoch of 10, 580512 pairs, loss: 1194996.625000\n",
      "8 epoch of 10, 580512 pairs, loss: 1158542.375000\n",
      "9 epoch of 10, 580512 pairs, loss: 1123906.125000\n",
      "10 epoch of 10, 580512 pairs, loss: 1091808.500000\n",
      "CPU times: user 1h 11min 54s, sys: 14min 18s, total: 1h 26min 13s\n",
      "Wall time: 6h 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d2v = SGNS(DVDataset(tagged_docs,min_count=0),dim=50,iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.1 s, sys: 158 ms, total: 8.26 s\n",
      "Wall time: 3.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "d2v_gensim = Doc2Vec(tagged_docs,min_count=0,dm=0,vector_size=50,negative=5,epochs=10,hs=0,alpha=0.025,min_alpha=0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate both models on Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def eval(model):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for i in range(len(train_reduced['target'])):\n",
    "        X_train.append(model[str(i)])\n",
    "        y_train.append(train_reduced['target'][i])\n",
    "    \n",
    "    for i in range(len(test_reduced['target'])):\n",
    "        X_test.append(model[str(i+len(train_reduced['target']))])\n",
    "        y_test.append(test_reduced['target'][i])\n",
    "        \n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    return clf.score(X_test,y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5928381962864722"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(d2v_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5822281167108754"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(d2v.get_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepwalk\n",
    "The SkipGram model has been adapted to Graphs [1], learning embeddings of nodes in the graph. The key idea here is that nodes that have a similar neighborhood should be close in embeddings space. That corresponds to embeddings of words that occur in a similar context.\n",
    "In order to achieve this goal, random walks are sampled from the graph and then treated as sentence equivalents.\n",
    "\n",
    "[1] Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. \"Deepwalk: Online learning of social representations.\" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download a graph dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  164k  100  164k    0     0  83649      0  0:00:02  0:00:02 --:--:-- 83649\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "!mkdir ./cora\n",
    "!curl -o ./cora/cora.tar.gz \"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\"\n",
    "!gzip -d < ./cora/cora.tar.gz | tar xf - --directory ./\n",
    "!rm ./cora/cora.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the description of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This directory contains the a selection of the Cora dataset (www.research.whizbang.com/data).\r\n",
      "\r\n",
      "The Cora dataset consists of Machine Learning papers. These papers are classified into one of the following seven classes:\r\n",
      "\t\tCase_Based\r\n",
      "\t\tGenetic_Algorithms\r\n",
      "\t\tNeural_Networks\r\n",
      "\t\tProbabilistic_Methods\r\n",
      "\t\tReinforcement_Learning\r\n",
      "\t\tRule_Learning\r\n",
      "\t\tTheory\r\n",
      "\r\n",
      "The papers were selected in a way such that in the final corpus every paper cites or is cited by atleast one other paper. There are 2708 papers in the whole corpus. \r\n",
      "\r\n",
      "After stemming and removing stopwords we were left with a vocabulary of size 1433 unique words. All words with document frequency less than 10 were removed.\r\n",
      "\r\n",
      "\r\n",
      "THE DIRECTORY CONTAINS TWO FILES:\r\n",
      "\r\n",
      "The .content file contains descriptions of the papers in the following format:\r\n",
      "\r\n",
      "\t\t<paper_id> <word_attributes>+ <class_label>\r\n",
      "\r\n",
      "The first entry in each line contains the unique string ID of the paper followed by binary values indicating whether each word in the vocabulary is present (indicated by 1) or absent (indicated by 0) in the paper. Finally, the last entry in the line contains the class label of the paper.\r\n",
      "\r\n",
      "The .cites file contains the citation graph of the corpus. Each line describes a link in the following format:\r\n",
      "\r\n",
      "\t\t<ID of cited paper> <ID of citing paper>\r\n",
      "\r\n",
      "Each line contains two paper IDs. The first entry is the ID of the paper being cited and the second ID stands for the paper which contains the citation. The direction of the link is from right to left. If a line is represented by \"paper1 paper2\" then the link is \"paper2->paper1\". "
     ]
    }
   ],
   "source": [
    "!cat cora/README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the graph with networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.read_edgelist('./cora/cora.cites')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a random walk, starting at a particular node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rand\n",
    "\n",
    "def random_walk(G,path_length, start=None):\n",
    "    path = [start]\n",
    "\n",
    "    while len(path) < path_length:\n",
    "        cur = path[-1]\n",
    "        neighbors = list(G.neighbors(cur))\n",
    "        if len(neighbors) > 0:\n",
    "            path.append(rand.choice(neighbors))\n",
    "        else:\n",
    "            break\n",
    "    return [str(node) for node in path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create several walks for each node in the graph\n",
    "Typical values for number of walks per node are 40 and the length of each walk 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_walks(G,num_paths,path_length):\n",
    "    walks = []\n",
    "    for cnt in range(num_paths):\n",
    "        for node in G.nodes:\n",
    "            walks.append(random_walk(G,path_length,node))\n",
    "    return walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed the generated walks into a Word2VecModel to create embeddings for the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = generate_walks(G,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 10.9 ms, total: 2.11 s\n",
      "Wall time: 844 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dw_gensim = Word2Vec(walks, vector_size=100, min_count=0,epochs=5,sg=1,hs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch of 5, 480698 pairs, loss: 1683031.125000\n",
      "2 epoch of 5, 482192 pairs, loss: 937517.375000\n",
      "3 epoch of 5, 481567 pairs, loss: 363318.906250\n",
      "4 epoch of 5, 481886 pairs, loss: 263049.406250\n",
      "5 epoch of 5, 481338 pairs, loss: 233549.500000\n",
      "CPU times: user 24min 48s, sys: 4min 57s, total: 29min 46s\n",
      "Wall time: 28min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dw = SGNS(WVDataset(walks,min_count=0),dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "def eval_dw(model,folds=10,test_size=0.1):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open('./cora/cora.content') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split()\n",
    "            y.append(line[-1])\n",
    "            X.append(model[line[0]])\n",
    "        clf = LogisticRegression(solver='liblinear',multi_class='ovr')\n",
    "        scores = []\n",
    "        for i in range(folds):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=test_size, random_state=i)\n",
    "            clf.fit(X_train,y_train)\n",
    "            score = metrics.f1_score(y_test,clf.predict(X_test), average='macro')\n",
    "            scores.append(score)\n",
    "        return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8192361404650276"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dw(dw_gensim.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8201304912010012"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dw(dw.get_embedding())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
